{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Student Cognitive Skills Analysis\n",
        "\n",
        "This notebook contains:\n",
        "1. Synthetic student dataset generation\n",
        "2. Correlation analysis between cognitive skills and performance\n",
        "3. ML model for assessment score prediction\n",
        "4. Student clustering into learning personas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic student data\n",
        "n_students = 1000\n",
        "\n",
        "# Generate random data for each feature\n",
        "student_data = {\n",
        "    'student_id': range(1, n_students + 1),\n",
        "    'name': [f'Student_{i}' for i in range(1, n_students + 1)],\n",
        "    'class': np.random.choice(['A', 'B', 'C', 'D'], n_students),\n",
        "    'comprehension': np.random.normal(70, 15, n_students).clip(0, 100),\n",
        "    'attention': np.random.normal(65, 20, n_students).clip(0, 100),\n",
        "    'focus': np.random.normal(75, 10, n_students).clip(0, 100),\n",
        "    'retention': np.random.normal(68, 18, n_students).clip(0, 100),\n",
        "    'engagement_time': np.random.normal(45, 15, n_students).clip(0, 90)  # in minutes\n",
        "}\n",
        "\n",
        "# Calculate assessment score based on cognitive skills\n",
        "weights = {\n",
        "    'comprehension': 0.3,\n",
        "    'attention': 0.2,\n",
        "    'focus': 0.2,\n",
        "    'retention': 0.2,\n",
        "    'engagement_time': 0.1\n",
        "}\n",
        "\n",
        "# Normalize engagement_time to 0-100 scale for score calculation\n",
        "engagement_normalized = (student_data['engagement_time'] - student_data['engagement_time'].min()) / \\\n",
        "                      (student_data['engagement_time'].max() - student_data['engagement_time'].min()) * 100\n",
        "\n",
        "# Calculate weighted score with some random noise\n",
        "base_score = (\n",
        "    weights['comprehension'] * student_data['comprehension'] +\n",
        "    weights['attention'] * student_data['attention'] +\n",
        "    weights['focus'] * student_data['focus'] +\n",
        "    weights['retention'] * student_data['retention'] +\n",
        "    weights['engagement_time'] * engagement_normalized\n",
        ")\n",
        "student_data['assessment_score'] = (base_score + np.random.normal(0, 5, n_students)).clip(0, 100)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(student_data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('../data/student_data.csv', index=False)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze correlations between cognitive skills and performance\n",
        "correlation_matrix = df[['comprehension', 'attention', 'focus', 'retention', 'engagement_time', 'assessment_score']].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix: Cognitive Skills vs Performance')\n",
        "plt.show()\n",
        "\n",
        "# Print key insights\n",
        "print(\"\\nKey Correlations with Assessment Score:\")\n",
        "for col in correlation_matrix.index[:-1]:\n",
        "    corr = correlation_matrix.loc[col, 'assessment_score']\n",
        "    print(f\"{col}: {corr:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build ML model to predict assessment scores\n",
        "X = df[['comprehension', 'attention', 'focus', 'retention', 'engagement_time']]\n",
        "y = df['assessment_score']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Print model performance\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.2f}\")\n",
        "\n",
        "# Print feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.coef_\n",
        "})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values('Importance', ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster students into learning personas\n",
        "# Select features for clustering\n",
        "X_cluster = df[['comprehension', 'attention', 'focus', 'retention', 'engagement_time']]\n",
        "X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
        "\n",
        "# Determine optimal number of clusters using elbow method\n",
        "inertias = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_cluster_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Plot elbow curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K, inertias, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# Perform clustering with optimal k=4\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(X_cluster_scaled)\n",
        "\n",
        "# Analyze cluster characteristics\n",
        "cluster_means = df.groupby('cluster')[['comprehension', 'attention', 'focus', 'retention', 'engagement_time', 'assessment_score']].mean()\n",
        "print(\"\\nCluster Characteristics:\")\n",
        "print(cluster_means)\n",
        "\n",
        "# Save processed data for dashboard\n",
        "df.to_csv('../data/processed_student_data.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
